1.curl : curl  is  a tool to transfer data from or to a server.

-s -> Silent or quiet mode. Don't show progress meter  or  error  mes‐
                sages.   Makes  Curl mute.

curl -s http://public-dns.info/nameserver/br.csv   :  retrieve and echo the csv file mentioned in url

2. cut :The cut command in UNIX is a command for cutting out the sections from each line of files.
-d,: to mention delimeter is ","
-f1:cut field no 1 of each line

cut -d, -f1 filename:cut each line into fields using delimeter "," and then output first fields(ip addresses) of each line on console

3.shuf filename:shuffle the lines(ip addresses) of a given input file randomly

4.
tail :extract few  lines from the last of file.

-n 50:give count of lines to be extracted from last(50)

tail -n 50 :extract 50  lines from the last of file.

5.
xargs :it is used to execute a command by passing constructed argument list.The arguments are typically a long list of filenames (generated by ls or find etc) that are passed to xargs via a pipe. 

-i:Replace occurrences of {} in the argument with line  read  from file.

timeout 1:kill the process mentioned after 1 sec

ping:This command takes as input the IP address or the URL and sends a data packet to the specified address with the message “PING” and get a response from the server/host 

-c1 -w 1:define the number of packets(1)  to send by -c switch and time in sec to wait for response(by -w switch)

{} : arguments list with lines read from file passed by pipe or input stream(list of ip addresses in this case)
 
xargs -i timeout 1 ping -c1 -w 1 {}:exceute the ping command using 1 request packet and 1 sec wait time to response multiple times with each arguments in argument list(ip addressess)

6.grep "time=" filename: print each line in file having word "time=" and also highlight the word in each line

7.awk '{print substr($7, 6, length($7)) " " substr($4, 1, length($4) -1)}' filename:print each line with a word which is a 7th field and starting from sixth charcter ending when there is space encounter or untill length of field 7 of each line(time taken by server to response in this case followed by a space and ip address of server)

8.sort -n filename: sort the lines in file using numeric sort(-n option for numeric sort) in ascending order. In this case ip address of server which  response faster will be above.

9.awk '{print $2 " " $1 "ms"}' filename:print second field follwed by space and then first field(ip addresses space response time)

10.head -n 10 filename: extract first 10 lines and print on console and -n to set no of lines to be extracted from the begining of file(print first 10  fastest connected servers)

  
output of final command:
curl -s http://public-dns.info/nameserver/br.csv | cut -d, -f1 | shuf | tail -n 50 | xargs -i timeout 1 ping -c1 -w 1 {} | grep "time=" | awk '{print substr($7, 6, length($7)) " " substr($4, 1, length($4) -1)}' | sort -n | awk '{print $2 " " $1 "ms"}' | head -n 10


It retrieve ip last 50 ip addresses from csv file and find out first 10 fastest connecting servers and arrange them in ascending order by the time taken to connect them



